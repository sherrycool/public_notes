# 谷歌——机器学习速成教程

https://developers.google.com/machine-learning/crash-course/generalization/video-lecture

# 机器学习简介



作为软件工程师，机器学习可以帮助什么：

1. 可缩短编程时间的工具。
2. 为特定的用户群体提供特定的程序。
3. 解决用人工方法解决而机器不能解决的问题，如人脸识别。

# 框架处理

什么是监督式机器学习？

机器学习系统：通过学习如何组合输入信息，来对从未见过的数据，做出有用的预测。

术语：标签和特征

标签：我们要预测的真实事物：y

- 基本线性回归中的y变量。

特征：用于描述数据的输入变量：xi

- 基本线性回归中的{x1,x2,...,xn}变量。

样本：数据的特定实例：x

有标签样本：具有{特征，标签}：{x,y}

- 用于训练模型

无标签样本：具有{特征，？}:{x,?}

- 用于对新数据做出预测

模型：可将样本映射到预测标签：y‘

- 由模型的内部参数定义，这些内部参数值是通过学习得到的。

# 深入理解机器学习

误差：L2损失。

目标：最大限度的减少整个数据集的误差。

机器学习建模思路：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为**经验风险最小化**。

损失：损失是对糟糕预测的惩罚。也就是说，**损失**是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。

训练模型：**训练**模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。

损失函数MSE（均方误差）：**均方误差** (**MSE**) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量。虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。

# 降低损失：reducing loss

降低损失的方法：迭代。

计算梯度。

参数空间的哪个方向：

- 计算梯度。计算导数。

利用迭代的方法，计算损失函数，然后找到损失最小的参数值。参数值按照一定的梯度变化，计算损失是变大还是变小。

# TensorFlow 简介

是一个神经网络的包。

```python
import tensorflow as tf

# Set up a linear classifier.
classifier = tf.estimator.LinearClassifier()

# Train the model on some example data.
classifier.train(input_fn=train_input_fn, steps=2000)

# Use it to predict.
predictions = classifier.predict(input_fn=predict_input_fn)
```

工具：在线编程（python）和笔记本：

https://colab.research.google.com/notebooks/welcome.ipynb#recent=true

机器学习常用超参数：

steps： 训练迭代中的总次数，一步计算一批样本产生的损失，然后使用该值修改模型的权重一次。（练习次数）

？ batch size：单步的样本数量（随机选择）

total number of trained examples = batch size * steps.

机器学习中的方便变量：

？ periods：控制报告的粒度。

number of training examples in each period = batch sieze * steps / periods.

## python数据基本处理方法：

pandas是表格处理，主要处理两种数据结构。

- data frame：行* 列
- series： 列（不需要是时间序列，就是个序列就好）

numpy是数字计算的包

```python
import pandas as pd
pd.__version__ # version版本是pandas的一个变量
pd.Series(['a','b','c']) # 创建序列
city_names = pd.Series(['San Francisco', 'San Jose', 'Sacramento'])
population = pd.Series([852469, 1015785, 485199])
pd.DataFrame({ 'City name': city_names, 'Population': population })　#　创建数据表
california_housing_dataframe = pd.read_csv("https://storage.googleapis.com/mledu-datasets/california_housing_train.csv", sep=",")　#　导入表
california_housing_dataframe.describe() # 简单统计表
california_housing_dataframe.head() # 显示表的前几行
california_housing_dataframe.hist('housing_median_age') # 画一个序列的图
cities = pd.DataFrame({ 'City name': city_names, 'Population': population }) # 创建表
type(cities['City name']) # 获得某类的数据类型
type(cities[0:2]) # 获得表格的类型
cities[0:2] # filter and select
population / 1000 # 计算数据

import numpy as np
np.log(population) # 计算
population.apply(lambda val: val > 1000000) # 我理解的lambda是专门用于apply计算中，每一个去判断的。
cities['Area square miles'] = pd.Series([46.87, 176.53, 97.92]) # 新增表格的一列
cities['Population density'] = cities['Population'] / cities['Area square miles']
cities['newline'] = (cities['Area square miles'] > 50) & cities['City name'].apply(lambda name:name.startswith('San')) # startswith是以什么开始的判断

city_names.index # 获得表格的索引值（每一行有一个值）（这是个序列）
cities.index # 获得表格的索引值（这是个数据框）
cities.reindex([2, 0, 1]) # 利用索引值排序

```

tensorflow是google开发的一个机器学习包，内容丰富，功能强大。

```python
import tensorflow as tf
with tf.Session():  # 创建tf格式的序列数据
  input1 = tf.constant(1.0, shape=[2, 3])
  input2 = tf.constant(np.reshape(np.arange(1.0, 7.0, dtype=np.float32), (2, 3)))
  output = tf.add(input1, input2) # 计算
  result = output.eval() # ？
  np.dot() # 线性代数内积（线性代数乘法）
  np.multiply # 对应元素相乘
```

lambda函数

总结：lambda 是为了减少单行函数的定义而存在的。

是一种简化的定义函数的方法。

```python
california_housing_dataframe["rooms_per_person"] = (
    california_housing_dataframe["rooms_per_person"]).apply(lambda x: min(x, 5))
# 这样可以接触一个数据框中，min（x,5）某一个变量在[0,5]之间的。 max(x,0)选择的是不小于0。
```

# 泛化

过拟合。

数据样本。

奥卡姆剃刀定律：模型越简单，良好的实证结果就越可能不仅仅基于样本的特性。

泛化理论：使用经验。

使用测试集方法：从该模型中重新取样，去看模型是否在新的数据样本中表现良好。

一般来说，在测试集中表现是否良好是衡量能否在新数据上表现来那个号的有用指标：

- 如果测试集足够大
- 如果我们不反复使用相同的测试集来做

机器学习细则：

1. 抽样要求是随机抽取独立同分布（iid）的样本
2. 分布是平稳的：分布不能随时间发生变化
3. 始终从同一分布中抽取样本：包括训练集、验证集和测试集