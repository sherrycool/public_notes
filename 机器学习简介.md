# 谷歌——机器学习速成教程

https://developers.google.com/machine-learning/crash-course/generalization/video-lecture

# 机器学习简介



作为软件工程师，机器学习可以帮助什么：

1. 可缩短编程时间的工具。
2. 为特定的用户群体提供特定的程序。
3. 解决用人工方法解决而机器不能解决的问题，如人脸识别。

# 框架处理

什么是监督式机器学习？

机器学习系统：通过学习如何组合输入信息，来对从未见过的数据，做出有用的预测。

术语：标签和特征

标签：我们要预测的真实事物：y

- 基本线性回归中的y变量。

特征：用于描述数据的输入变量：xi

- 基本线性回归中的{x1,x2,...,xn}变量。

样本：数据的特定实例：x

有标签样本：具有{特征，标签}：{x,y}

- 用于训练模型

无标签样本：具有{特征，？}:{x,?}

- 用于对新数据做出预测

模型：可将样本映射到预测标签：y‘

- 由模型的内部参数定义，这些内部参数值是通过学习得到的。

# 深入理解机器学习

误差：L2损失。

目标：最大限度的减少整个数据集的误差。

机器学习建模思路：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为**经验风险最小化**。

损失：损失是对糟糕预测的惩罚。也就是说，**损失**是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。

训练模型：**训练**模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。

损失函数MSE（均方误差）：**均方误差** (**MSE**) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量。虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。

# 降低损失：reducing loss

降低损失的方法：迭代。

计算梯度。

参数空间的哪个方向：

- 计算梯度。计算导数。

利用迭代的方法，计算损失函数，然后找到损失最小的参数值。参数值按照一定的梯度变化，计算损失是变大还是变小。

# TensorFlow 简介

是一个神经网络的包。

```python
import tensorflow as tf

# Set up a linear classifier.
classifier = tf.estimator.LinearClassifier()

# Train the model on some example data.
classifier.train(input_fn=train_input_fn, steps=2000)

# Use it to predict.
predictions = classifier.predict(input_fn=predict_input_fn)
```

工具：在线编程（python）和笔记本：

https://colab.research.google.com/notebooks/welcome.ipynb#recent=true

机器学习常用超参数：

steps： 训练迭代中的总次数，一步计算一批样本产生的损失，然后使用该值修改模型的权重一次。（练习次数）

？ batch size：单步的样本数量（随机选择）

total number of trained examples = batch size * steps.

机器学习中的方便变量：

？ periods：控制报告的粒度。

number of training examples in each period = batch sieze * steps / periods.

## python数据基本处理方法：

pandas是表格处理，主要处理两种数据结构。

- data frame：行* 列
- series： 列（不需要是时间序列，就是个序列就好）

numpy是数字计算的包

```python
import pandas as pd
pd.__version__ # version版本是pandas的一个变量
pd.Series(['a','b','c']) # 创建序列
city_names = pd.Series(['San Francisco', 'San Jose', 'Sacramento'])
population = pd.Series([852469, 1015785, 485199])
pd.DataFrame({ 'City name': city_names, 'Population': population })　#　创建数据表
california_housing_dataframe = pd.read_csv("https://storage.googleapis.com/mledu-datasets/california_housing_train.csv", sep=",")　#　导入表
california_housing_dataframe.describe() # 简单统计表
california_housing_dataframe.head() # 显示表的前几行
california_housing_dataframe.hist('housing_median_age') # 画一个序列的图
cities = pd.DataFrame({ 'City name': city_names, 'Population': population }) # 创建表
type(cities['City name']) # 获得某类的数据类型
type(cities[0:2]) # 获得表格的类型
cities[0:2] # filter and select
population / 1000 # 计算数据

import numpy as np
np.log(population) # 计算
population.apply(lambda val: val > 1000000) # 我理解的lambda是专门用于apply计算中，每一个去判断的。
cities['Area square miles'] = pd.Series([46.87, 176.53, 97.92]) # 新增表格的一列
cities['Population density'] = cities['Population'] / cities['Area square miles']
cities['newline'] = (cities['Area square miles'] > 50) & cities['City name'].apply(lambda name:name.startswith('San')) # startswith是以什么开始的判断

city_names.index # 获得表格的索引值（每一行有一个值）（这是个序列）
cities.index # 获得表格的索引值（这是个数据框）
cities.reindex([2, 0, 1]) # 利用索引值排序

```

tensorflow是google开发的一个机器学习包，内容丰富，功能强大。

```python
import tensorflow as tf
with tf.Session():  # 创建tf格式的序列数据
  input1 = tf.constant(1.0, shape=[2, 3])
  input2 = tf.constant(np.reshape(np.arange(1.0, 7.0, dtype=np.float32), (2, 3)))
  output = tf.add(input1, input2) # 计算
  result = output.eval() # ？
  np.dot() # 线性代数内积（线性代数乘法）
  np.multiply # 对应元素相乘
```

lambda函数

总结：lambda 是为了减少单行函数的定义而存在的。

是一种简化的定义函数的方法。

```python
california_housing_dataframe["rooms_per_person"] = (
    california_housing_dataframe["rooms_per_person"]).apply(lambda x: min(x, 5))
# 这样可以接触一个数据框中，min（x,5）某一个变量在[0,5]之间的。 max(x,0)选择的是不小于0。
```

# 泛化
（不明白泛化和过拟合的关系）

过拟合。过拟合的特点，就是在训练数据中损失很低，但是在预测新数据中表现草稿。

数据样本。

奥卡姆剃刀定律：模型越简单，良好的实证结果就越可能不仅仅基于样本的特性。

泛化理论：使用经验。

使用测试集方法：从该模型中重新取样，去看模型是否在新的数据样本中表现良好。

一般来说，在测试集中表现是否良好是衡量能否在新数据上表现来那个号的有用指标：

- 如果测试集足够大
- 如果我们不反复使用相同的测试集来做

机器学习细则：

1. 抽样要求是随机抽取独立同分布（iid）的样本
2. 分布是平稳的：分布不能随时间发生变化
3. 始终从同一分布中抽取样本：包括训练集、验证集和测试集

# 训练集和测试集（Training and Test Sets)
如果我们只有一个数据集，把两个分成两个数据集，一个用于训练，一个用于测试。
*请勿对测试数据进行训练!*
 训练集：用于训练模型的子集。
 测试集：用于测试训练后模型的子集。
 
 测试集满足的条件：
 1. 规模足够大，可产生具有统计意义的结果。
 2. 能代表整个数据集。测试集的特征和应该与训练集的特征相同。
 
 新方法：训练集、测试集、验证集。
 方法是：
 1. 在训练集训练模型
 2. 使用验证集评估模型
 3. 根据在验证集上获得的效果调整模型
 （迭代）
4. 选择在验证集上获得最佳效果的模型
5. 使用测试集确认模型的效果

练习：画图
```python
from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt # 画图包

plt.figure(figsize=(13, 8)) # 布置幕布

ax = plt.subplot(1, 2, 1) # 一幅图里面画几个图：1，2，1就是一行两列=1*2 两张图，最后一个1表示现在是第一张图。
ax.set_title("Validation Data") # 图的标题

ax.set_autoscaley_on(False) # y轴标尺范围是否自动
ax.set_ylim([32, 43]) # y轴范围
ax.set_autoscalex_on(False)
ax.set_xlim([-126, -112])
plt.scatter(validation_examples["longitude"],  #散点图，x轴
             validation_examples["latitude"],  #散点图，y轴
            cmap="coolwarm", # 色彩
            c=validation_targets["median_house_value"] / validation_targets["median_house_value"].max()) # 气泡颜色-气泡图

ax = plt.subplot(1,2,2) #设置第二张图，也就是最后一个2.
ax.set_title("Training Data")

ax.set_autoscaley_on(False)
ax.set_ylim([32, 43])
ax.set_autoscalex_on(False)
ax.set_xlim([-126, -112])
plt.scatter(training_examples["longitude"],
            training_examples["latitude"],
            cmap="coolwarm",
            c=training_targets["median_house_value"] / training_targets["median_house_value"].max())
_ = plt.plot() # 不明白为啥是这样的
```

 tensorflow 特征工程处理：feature_column
 在使用很多模型的时候，都需要对输入的数据进行必要的特征工程处理。最典型的就是:one-hot处理，还有hash分桶等处理。为了方便处理这些特征，tensorflow提供了一些列的特征工程方法来方便使用.
 http://blog.csdn.net/cjopengler/article/details/78161748
 特征工程的方法有：
- numeric_column: 主要是正态化。
- bucketized_column：分桶。
- categorical_column_with_vocabulary_list： 返回稀疏的tensor。
- categorical_column_with_identity：对连续型数字处理。
- embedding_column： 将sparsor tensor处理为dense tensor。
- weighted_categorical_column： 
- linear_model:对所有特定进行线性加权操作。
- crossed_column:适用于sparse特征。
 
机器学习的例子：加州房价
一个特征变量：
```python
# 载入加载项
import math 

from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
import tensorflow as tf
from tensorflow.python.data import Dataset

tf.logging.set_verbosity(tf.logging.ERROR)
pd.options.display.max_rows = 10 # 显示10行数据
pd.options.display.float_format = '{:.1f}'.format # 显示数字为一位小数
# 载入数据
california_housing_dataframe = pd.read_csv("https://storage.googleapis.com/mledu-datasets/california_housing_train.csv", sep=",")
# 数据清理和处理
california_housing_dataframe = california_housing_dataframe.reindex(
    np.random.permutation(california_housing_dataframe.index)) # 重新排序（应该是打乱了）
california_housing_dataframe["median_house_value"] /= 1000.0 # 修改房价中位数的单位。
california_housing_dataframe # 显示数据
# 展示数据
california_housing_dataframe.describe()
# 开始机器学习
# 第一步，定义特征并配置特征列
# Define the input feature: total_rooms.定义input feature（输入的特征）
my_feature = california_housing_dataframe[["total_rooms"]] # 一个特征，一列-房间数（一维数组）

# Configure a numeric feature column for total_rooms.
feature_columns = [tf.feature_column.numeric_column("total_rooms")] # 应该是定义特征列，但我不认为这里已经有数据了。
# 第二步，定义目标（label-标签，结果变量）
# Define the label.
targets = california_housing_dataframe["median_house_value"]
# 第三步，配置线性回归LinearRegressor，并使用 GradientDescentOptimizer（它会实现小批量随机梯度下降法 (SGD)）训练该模型。learning_rate 参数可控制梯度步长的大小。
# 这个线性回归和我在R调公式出来的结果不同？这里有计算过程。为什么要设置步长呢？
# 注意：为了安全起见，我们还会通过 clip_gradients_by_norm 将梯度裁剪应用到我们的优化器。梯度裁剪可确保梯度大小在训练期间不会变得过大，梯度过大会导致梯度下降法失败。
# Use gradient descent as the optimizer for training the model.
my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001) # 定义optimizer（执行算法的处理器），梯度下降法，每次步长为0.0000001
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0) # 定义optimizer的另一个属性，优化梯度变化的幅度。

# Configure the linear regression model with our feature columns and optimizer.
# Set a learning rate of 0.0000001 for Gradient Descent.
linear_regressor = tf.estimator.LinearRegressor(   # 定义线性回归的算法
    feature_columns=feature_columns,  # 前面有设置特征 
    optimizer=my_optimizer
)


```

多个特征变量：
